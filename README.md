# Bank-Deposit Prediction Project üí∞üîÆ

A compact, end-to-end **binary-classification** workflow that predicts whether a prospective client of a Portuguese bank will subscribe to a term deposit.  
The notebook shows the full path from raw data to a tuned model‚Äîideal as a template for structured-tabular ML work.

---

## Folder layout

```

bank\_deposit\_prediction\_project/
‚îú‚îÄ‚îÄ dataset/
‚îÇ   ‚îî‚îÄ‚îÄ bank-full.csv           # 45 211 rows √ó 17 features (UCI Bank Marketing)
‚îú‚îÄ‚îÄ Bank\_Deposit\_Prediction\_Project.ipynb
‚îî‚îÄ‚îÄ Project Report.pdf          # short paper-style write-up

````
*The raw CSV is the ‚Äú**bank-full**‚Äù version of the Bank-Marketing dataset*

---

## Quick start

```bash
git clone https://github.com/CarloH-AI/bank_deposit_prediction_project.git
cd bank_deposit_prediction_project

# optional: clean Python env
python -m venv .venv && source .venv/bin/activate   # (Windows: .venv\Scripts\activate)
pip install -r requirements.txt      # or use the minimal list below

# open the notebook
jupyter lab Bank_Deposit_Prediction_Project.ipynb
````

**Minimal requirements**

```
python >= 3.10
pandas numpy scikit-learn imbalanced-learn matplotlib seaborn xgboost jupyter
```

---

## Pipeline in a nutshell

| Step                   | Key actions                                                                                                                |
| ---------------------- | -------------------------------------------------------------------------------------------------------------------------- |
| **1 ‚Äì EDA**            | Class balance check (‚âà 11 % positive), feature inspection, correlation heat-map.                                           |
| **2 ‚Äì Pre-processing** | One-hot encode categoricals, scale continuous vars, handle class imbalance with **SMOTE**.                                 |
| **3 ‚Äì Model sweep**    | Benchmark **Logistic Regression**, **Random Forest**, and **XGBoost**; 5-fold cross-validation over hyper-parameter grids. |
| **4 ‚Äì Evaluation**     | Metrics: ROC-AUC, F1, Recall; plots: ROC curve and Precision-Recall.                                                       |
| **5 ‚Äì Findings**       | Tuned **XGBoost** reaches ‚âà 0.92 ROC-AUC on the hold-out set; top drivers are *duration*, *poutcome*, and *contact type*.  |
| **6 ‚Äì Next steps**     | SHAP-based interpretability, cost-sensitive thresholds, pipeline export with `skops`.                                      |

Dataset reference: UCI Bank Marketing (45 211 records, 17 features, target **y**) ([archive.ics.uci.edu][1])

---

## Results at a glance

* **Best model** ‚Äì XGBoost (learning-rate 0.05, 300 rounds, `max_depth=5`)
* **Hold-out ROC-AUC** ‚Äì \~0.92
* **Lift\@10 %** ‚Äì \~2.8√ó over random (see notebook section ‚ÄúBusiness lift‚Äù)

> The model improves recall on the minority ‚Äúyes‚Äù class by applying SMOTE and threshold tuning, while maintaining precision suitable for targeted follow-ups.

[1]: https://archive.ics.uci.edu/dataset/222/bank%2Bmarketing "UCI Machine Learning Repository"
